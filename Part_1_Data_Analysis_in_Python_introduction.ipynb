{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 1 Data Analysis in Python introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nIfaR31T8Qcb",
        "Lo-AFouA1UIo",
        "z4jt2PbVzqVB",
        "pasaP5RbyExW",
        "LaYmywfvfN8a",
        "2f1EIQk41_cP",
        "66BLClud3Onb",
        "xZspxb6M4JW5"
      ],
      "mount_file_id": "https://github.com/HansHenseler/masdav2023/blob/main/Part_1_Data_Analysis_in_Python_introduction.ipynb",
      "authorship_tag": "ABX9TyM9B9bFKHMgQzshAXeGc7NA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HansHenseler/masdav2024/blob/main/Part_1_Data_Analysis_in_Python_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLdnjTDw2FZv"
      },
      "source": [
        "# Python for Data Analysis introduction\n",
        "\n",
        "Master of Advanced Studies in Digital Forensics & Cyber Investigation\n",
        "\n",
        "Data Analytics and Visualization for Digital Forensics\n",
        "\n",
        "(c) Hans Henseler, 2024\n",
        "\n",
        "This exercise starts with examples of most common data analysis tasks with Python, from the features of Python itself to using modules like Pandas with a few exercises that you can try to accomplish yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP-a0acO_lMw"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8bNCImD3PLl"
      },
      "source": [
        "## 1 A Note About Python Versions\n",
        "All examples in this cheat sheet use Python 3. We recommend using the latest stable version of Python, for example, Python 3.8. You can check which version you have installed on your machine by running the following command in the system shell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qUgB5w-12ev"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgU_-2SGXpL3"
      },
      "source": [
        "# We need some files with sample data for the examples and excersises this morning.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jExGb4aA3sLG"
      },
      "source": [
        "## 2 Libraries and Imports\n",
        "The easiest way to install Python modules that are needed for data analysis is to use pip. Installing NumPy and Pandas takes only a few seconds. In colab these libraries come pre installed. Once youâ€™ve installed the modules, use the import statement to make the modules available in your program:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ReZzLXr38Q5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzeElMYt4IV-"
      },
      "source": [
        "## 3 Getting Help With Python Data Analysis Functions\n",
        "If you get stuck,the Google Colab interface offers context sensitive help when you are typing code. In other situations the built-in Python docs are a great place to check for tips and ways to solve the problem. The Python help() function displays the help article for a method or a class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8qQZe3g5McT"
      },
      "source": [
        "## 4 Working with data sources\n",
        "Pandas provides a number of easy-to-use data import methods, including CSV and TSV import, copying from the system clipboard, and reading and writing JSON files. This is sufficient for most Python data analysis tasks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVbC5_xP4opF"
      },
      "source": [
        "# Connect your drive to the Testdata folder that you have requested permission for.\n",
        "# Check if the path to the file \"DirPrint_Filelist - clean.xlsx\" is valid. If not correct it\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Testdata/DirPrint_Filelist - clean.xlsx')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSq8gkmz6HYl"
      },
      "source": [
        "## 5 Working with Pandas Data Frames\n",
        "Pandas data frames are a great way to explore, clean, tweak, and filter your data sets while doing data analysis in Python. This section covers a few of the things you can do with your Pandas data frames.\n",
        "\n",
        "# Exploring data\n",
        "Here are a few functions that allow you to easily know more about the data set you are working on:#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQSx3mXIjs16"
      },
      "source": [
        "# show the data types of the columns\n",
        "#\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsGLf3735_Hn"
      },
      "source": [
        "# show the number of columns and rows\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqKqswTg6wjx"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmOM842n668Q"
      },
      "source": [
        "df[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUD5GmFN8HgB"
      },
      "source": [
        "df[2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxZ6R4I88JRl"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rV_OnKn8MmX"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyGETnkcIQyE"
      },
      "source": [
        "# we can also create a new column that is based on another colum\n",
        "# The following code splits the path based on \\ and stores the array in a new column called Folders\n",
        "df['Folders']=df['Path'].str.split('\\\\')\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIfaR31T8Qcb"
      },
      "source": [
        "## 6 Statistical operations\n",
        "All standard statistical operations like minimums, maximums, and custom quantiles are present in Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faLVtfQ78RmR"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0A0ODsO8Wjd"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkqO91W88Yft"
      },
      "source": [
        "df[\"Size\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7gPwEmF8enn"
      },
      "source": [
        "df[\"Size\"].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo7Mr4ID8hh8"
      },
      "source": [
        "df[\"Size\"].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUh4EEyk8kX0"
      },
      "source": [
        "df[\"Size\"].quantile(0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2tNNXId8ryC"
      },
      "source": [
        "df[\"Size\"].quantile(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5bNYl7_BJnI"
      },
      "source": [
        "df[\"Size\"].hist(bins=[0,100,150,100000,1000000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Size\"].value_counts()"
      ],
      "metadata": {
        "id": "Te9lk6vWokLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can examine all values of Size by resetting the number of max_rows that are displayed\n",
        "# pd.options.display.max_rows = None\n",
        "# Restore this\n",
        "# pd.reset_option('display.max_rows')\n",
        "# The following bin sizes make more sense for this data set but the first bin\n",
        "# is still relatively full\n",
        "\n",
        "df[\"Size\"].hist(bins=[49, 8390, 16731, 25073, 33414, 41756, 50097, 58439, 66780, 75121,\n",
        " 83463, 91804, 100146, 108487, 116829, 125170, 133512])"
      ],
      "metadata": {
        "id": "MVt0MKH9t5_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ILT1Vf84a5"
      },
      "source": [
        "## 7 Cleaning the Data\n",
        "It is quite common to have not-a-number (NaN) values in your data set. To be able to operate on a data set with statistical methods, youâ€™ll first need to clean up the data. The fillna and dropna Pandas functions are a convenient way to replace the NaN values with something more representative for your data set, for example, a zero, or to remove the rows with NaN values from the data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-yQqM9pU__S"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxBgPr1C86GI"
      },
      "source": [
        "clean_df = df\n",
        "clean_df = df[\"Attributes\"].fillna(\"Unknown\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYLgUFz8Jevw"
      },
      "source": [
        "clean_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pilMARqH9AND"
      },
      "source": [
        "df['Attributes'].dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1AnnVNb9D4B"
      },
      "source": [
        "df.replace('---A----','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo-AFouA1UIo"
      },
      "source": [
        "## 8 Reading raw text data\n",
        "\n",
        "In step 4 we read an xlsx file. Read_xlsx nicely formats our data. This is not always the case. As an example there is a small section from the dirprint file which is formatted as tab delimited test. This file has no column names but the columns are the same as in the xlsx file used above: Attributes,Created, LastMod, LastAcc, Size, Name, Path, Folder, Ext, Md5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJidOsdF2WNL"
      },
      "source": [
        "# The DirPrint_Filelist_small.txt is just a small fragement of the xlsx file we used earlier\n",
        "# It contains tab delimited data and has no headers. This is a good way to start\n",
        "#\n",
        "dfs = pd.read_csv('/content/drive/MyDrive/Testdata/DirPrint_Filelist_small.txt',sep='\\t')\n",
        "dfs.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBniZsiI2wMh"
      },
      "source": [
        "# Compare this to the dtypes we got from read_excel (see #3 above):\n",
        "#\n",
        "# Attributes            object\n",
        "# Created       datetime64[ns]\n",
        "# LastMod       datetime64[ns]\n",
        "# LastAcc       datetime64[ns]\n",
        "# Size                   int64\n",
        "# Name                  object\n",
        "# Path                  object\n",
        "# Folder                object\n",
        "# Ext                   object\n",
        "# Md5                   object\n",
        "#\n",
        "# What's wrong?\n",
        "#\n",
        "# hint check the python docs https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkgyefs0ocxx"
      },
      "source": [
        "# Add column headers\n",
        "#\n",
        "dfs.columns = ['Attributes','Created','LastMod','LastAcc','Size','Name','Path','Folder','Ext','Md5']\n",
        "dfs.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "708IVuysohdQ"
      },
      "source": [
        "# Make the Size column an int64 (we need to take care of the , thousands separator)\n",
        "#\n",
        "dfs['Size'] = dfs.Size.astype(str).str.replace(',', '').astype(np.int64)\n",
        "dfs.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L35D0z1soq9d"
      },
      "source": [
        "# Parse the dates\n",
        "dfs['Created'] = pd.to_datetime(dfs['Created'])\n",
        "dfs['LastMod'] = pd.to_datetime(dfs['LastMod'])\n",
        "dfs['LastAcc'] = pd.to_datetime(dfs['LastAcc'])\n",
        "dfs.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_86h2aLrrzX"
      },
      "source": [
        "# With these operations we have now cleaned/formatted the tab delimited file\n",
        "# in come cases we want a column the be the index of the data frame.\n",
        "#\n",
        "dfs = dfs.set_index(\"LastMod\")\n",
        "dfs.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3pL_7p7LoyL"
      },
      "source": [
        "dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI7zU6IXnyLZ"
      },
      "source": [
        "# If you need to repeat this process for multiple files it is also possible to\n",
        "# specify this in the call to read_csv as follows\n",
        "#\n",
        "from datetime import datetime\n",
        "\n",
        "# mydateparser = lambda x: pd.datetime.strptime(x, '%m/%d/%Y %H:%M')\n",
        "mydateparser = lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M')\n",
        "\n",
        "dfs = pd.read_csv(\n",
        "    '/content/drive/MyDrive/Testdata/DirPrint_Filelist_small.txt',\n",
        "    sep=\"\\t\",usecols=[0,1,2,3,4,5,6,7,8,9],\n",
        "    names=['Attributes','Created','LastMod','LastAcc','Size','Name','Path','Folder','Ext','Md5'],\n",
        "    index_col='LastMod',\n",
        "    header=0,\n",
        "    dtype={'Size':np.int64},\n",
        "    thousands=',',\n",
        "    parse_dates = ['Created', 'LastMod','LastAcc'],\n",
        "    date_parser=mydateparser\n",
        "    )\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szb6BXQ3MMBL"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwUSNoMQtWPA"
      },
      "source": [
        "# the datetime type also supports extracting year, month and day of the month\n",
        "\n",
        "pd.DatetimeIndex(dfs['Created']).year"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TkNhOqjuzsu"
      },
      "source": [
        "pd.DatetimeIndex(dfs['Created']).month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0q_G7qTu2Np"
      },
      "source": [
        "pd.DatetimeIndex(dfs['Created']).day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj70oN0Hu5iz"
      },
      "source": [
        "# and even week\n",
        "#\n",
        "pd.DatetimeIndex(dfs['Created']).week"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Qsvsjv0ZZT"
      },
      "source": [
        "# and even week\n",
        "#\n",
        "pd.DatetimeIndex(dfs['Created']).week"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24dM1ibY0bpM"
      },
      "source": [
        "# and even week (although the method week() was depricated and we have to use isocalendar() in stead)\n",
        "#\n",
        "#pd.DatetimeIndex(dfs['Created']).week\n",
        "pd.DatetimeIndex(dfs['Created']).isocalendar().week"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvr4LJqHIAyY"
      },
      "source": [
        "## 9 Filtering and sorting\n",
        "Here are some basic commands for filtering and sorting the data in your data frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-tJtATUICZD"
      },
      "source": [
        "df.sort_values(by=['Name','Size'],ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejz-UR5rIRZq"
      },
      "source": [
        "df.query('Size>1000000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVTGXkhnIffm"
      },
      "source": [
        "# use column names as Python attributes to filter with multiple clauses\n",
        "df[(df.Size>100000) & (df.Size<1000000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtO9lMeHJQY9"
      },
      "source": [
        "# or access columns with the df[] syntax\n",
        "df[(df['Size']>100000) & (df['Size']<1000000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLL8Ykir1UDK"
      },
      "source": [
        "# we can use regular expressions to filter through the data. For more\n",
        "# information see: https://docs.python.org/3/howto/regex.html\n",
        "#\n",
        "# For example we want to select all files with name System.*.dll where\n",
        "# * is not white space. In regex this is denoted by \\S\n",
        "#\n",
        "df[df.Name.str.match(r'(System[\\S]+.dll)')==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4jt2PbVzqVB"
      },
      "source": [
        "## 10 Generating pivot tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb1PcKgDJj5v"
      },
      "source": [
        "# generate a pivot tabel listing the number of files and total size per extension\n",
        "#\n",
        "df.pivot_table(index='Ext',values='Size',aggfunc=['sum','count'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seWjiUzyZP2S"
      },
      "source": [
        "# There are actually 5 entries for zfsendtotarget. The Ext column has data in lower case and upper case\n",
        "#\n",
        "df[df.Ext=='ZFSendToTarget']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO-BSPSWZ50W"
      },
      "source": [
        "# in order to change to lower case we have to convert the object type of column Ext to string\n",
        "df['Ext']=df['Ext'].astype('|S').str.decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs4iU4m-PeWZ"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAOutQ9KeQb-"
      },
      "source": [
        "# now we can change the Ext colum to lower case\n",
        "df['Ext']=df['Ext'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n80LBVA2eZTx"
      },
      "source": [
        "# let's create the same pivot table as before\n",
        "df.pivot_table(index='Ext',values='Size',aggfunc=['sum','count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3fonOoRe6Pe"
      },
      "source": [
        "# the number of entries in our pivot table has been reduced to 1024 and the zfsendtotarget increased from 4 to 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pasaP5RbyExW"
      },
      "source": [
        "## 11 Analysing and removing duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBBSnjOq4LuQ"
      },
      "source": [
        "# With the dataframe duplicated method we can detect duplicates in a panda dataframe\n",
        "#\n",
        "# also see: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html\n",
        "\n",
        "df.duplicated('Name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZEhvKC0yDhg"
      },
      "source": [
        "# the original df has 255831 rows. Here is how we can drop rows with duplicate file names\n",
        "# using the drop_duplicates method\n",
        "#\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html\n",
        "#\n",
        "df.drop_duplicates('Name', keep='last').shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBiAX99yXAy"
      },
      "source": [
        "# This removed more than half of our rows. We can also be more specific by defining duplicates based on multiple columns:\n",
        "#\n",
        "df.drop_duplicates(['Name','Size',], keep='last').shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COuYu3VB4eWx"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaYmywfvfN8a"
      },
      "source": [
        "## 1 Use a regular expression to select office files based on file extension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L70aaaGafhN2"
      },
      "source": [
        "# Your answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f1EIQk41_cP"
      },
      "source": [
        "## 2 Remove duplicate entries in df based on the md5 value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh8EeJNV3OEr"
      },
      "source": [
        "# your answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66BLClud3Onb"
      },
      "source": [
        "## 3 For a given file, find if it has duplicates and list the folders where duplicates are located"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzPBh5_F2Edy"
      },
      "source": [
        "# your answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZspxb6M4JW5"
      },
      "source": [
        "## 4 Create a pivot table showing number of files per month based on data last modified"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[ ] # your answer"
      ],
      "metadata": {
        "id": "hfzaSb8fhW-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YgF2hkdw-Uy"
      },
      "source": [
        "## 5 Create a pivot table listing number of files vs file extension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX8iMHjU48Lb"
      },
      "source": [
        "# your answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcoPwxOJvtIJ"
      },
      "source": [
        "## 6 Make a DirPrint file from your own computer\n",
        "\n",
        "The DirPrint xlsx file was created with Karen's DirPrinter. This tool can be downloaded from:\n",
        "\n",
        "https://www.karenware.com/powertools/karens-directory-printer\n",
        "\n",
        "Download and install DirPrinter and use it to examine your own computer.\n",
        "Select your own user folder and don't forget to check the box for subfolders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD5VDTzgvrn9"
      },
      "source": [
        "# your answer"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}